#!/usr/bin/env python3
"""
Test streaming output directly
"""

import asyncio
import sys
import os

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from fortune_teller.tools.llm_tool import LLMTool
from fortune_teller.core.config_manager import ConfigManager

async def test_streaming():
    """Test streaming LLM output"""
    
    print("ğŸ”® Testing Streaming Output...")
    print("=" * 50)
    
    # Initialize LLM tool
    llm_tool = LLMTool()
    
    # Test prompt
    system_prompt = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å…«å­—å‘½ç†å¸ˆï¼Œç²¾é€šä¼ ç»Ÿå‘½ç†å­¦ã€‚"
    user_prompt = """
è¯·ä¸ºä»¥ä¸‹å…«å­—ä¿¡æ¯æä¾›ç®€çŸ­çš„å‘½ç†åˆ†æï¼š

å…«å­—ï¼šåºšåˆ ç”²ç”³ ç™¸ä¸‘ å·±æœª
æ—¥ä¸»ï¼šç™¸æ°´
äº”è¡Œï¼šæœ¨1 ç«1 åœŸ3 é‡‘2 æ°´1

è¯·æä¾›æ€§æ ¼ç‰¹ç‚¹å’Œè¿åŠ¿å»ºè®®ã€‚
"""
    
    print("ğŸ¯ Generating reading with streaming...")
    print("ğŸ“ You should see text appear word by word:\n")
    
    try:
        # Test streaming output
        result = await llm_tool.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            language="zh",
            stream=True  # Enable streaming
        )
        
        print(f"\n\nâœ… Streaming completed!")
        print(f"ğŸ“Š Total response length: {len(result)} characters")
        
    except Exception as e:
        print(f"âŒ Error during streaming: {e}")
        
        # Fallback to non-streaming
        print("\nğŸ”„ Trying non-streaming mode...")
        result = await llm_tool.generate_response(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
            language="zh",
            stream=False
        )
        print(result)

if __name__ == "__main__":
    asyncio.run(test_streaming())
