#!/usr/bin/env python3
"""
Simple streaming test - directly test the LLM streaming
"""

import asyncio
import sys
import os

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

async def test_streaming():
    """Test streaming output directly"""
    
    print("ğŸ”® Testing Streaming Output")
    print("=" * 40)
    
    # Import after path setup
    from fortune_teller.core.aws_connector import AWSBedrockConnector
    
    try:
        # Create connector
        connector = AWSBedrockConnector()
        
        # Test prompts
        system_prompt = "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„å…«å­—å‘½ç†å¸ˆã€‚è¯·æä¾›ç®€æ´çš„å‘½ç†åˆ†æã€‚"
        user_prompt = """
è¯·ä¸ºä»¥ä¸‹ä¿¡æ¯æä¾›å…«å­—å‘½ç†åˆ†æï¼š

å‡ºç”Ÿä¿¡æ¯ï¼š1990å¹´7æœˆ25æ—¥ 14:30 ç”·æ€§
å…«å­—ï¼šåºšåˆ ç”²ç”³ ç™¸ä¸‘ å·±æœª
æ—¥ä¸»ï¼šç™¸æ°´

è¯·åˆ†ææ€§æ ¼ç‰¹ç‚¹å’Œè¿åŠ¿å»ºè®®ï¼Œä¿æŒç®€æ´ã€‚
"""
        
        print("ğŸ¯ Starting streaming generation...")
        print("ğŸ“ You should see text appear gradually:\n")
        print("-" * 40)
        
        # Test streaming
        full_response = ""
        for chunk in connector.generate_response_streaming(system_prompt, user_prompt):
            full_response += chunk
            print(chunk, end='', flush=True)
        
        print("\n" + "-" * 40)
        print(f"âœ… Streaming completed!")
        print(f"ğŸ“Š Total length: {len(full_response)} characters")
        
    except Exception as e:
        print(f"âŒ Streaming failed: {e}")
        print("ğŸ”„ This might be expected if no LLM is configured")

if __name__ == "__main__":
    asyncio.run(test_streaming())
