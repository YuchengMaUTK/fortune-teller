"""
LLM å·¥å…· - æä¾›å¤§è¯­è¨€æ¨¡å‹æœåŠ¡
"""

from typing import Dict, Any, Optional, List
import logging
from .base_tool import BaseTool
from ..ui.colors import Colors

logger = logging.getLogger(__name__)


class LLMTool(BaseTool):
    """
    LLM å·¥å…·ç±» - é›†æˆç°æœ‰çš„ LLM è¿æ¥å™¨
    """
    
    def __init__(self):
        super().__init__("llm_tool")
        self.description = "å¤§è¯­è¨€æ¨¡å‹æœåŠ¡å·¥å…·"
        self.llm_connector = None
    
    async def _setup(self) -> None:
        """åˆå§‹åŒ– LLM è¿æ¥å™¨"""
        try:
            from ..core.llm_connector import LLMConnector
            
            # ä½¿ç”¨é»˜è®¤é…ç½®åˆå§‹åŒ–è¿æ¥å™¨
            config = {
                "provider": "aws_bedrock",  # ä½¿ç”¨ AWS Bedrock
                "model": "anthropic.claude-3-5-haiku-20241022-v1:0",
                "temperature": 0.7,
                "max_tokens": 2000
            }
            
            self.llm_connector = LLMConnector(config)
            self.logger.info("LLM connector initialized successfully")
            
        except Exception as e:
            self.logger.error(f"Failed to initialize LLM connector: {e}")
            self.llm_connector = None
    
    async def generate_response(self, 
                              system_prompt: str, 
                              user_prompt: str, 
                              language: str = "zh",
                              provider: str = "auto",
                              stream: bool = False,
                              **kwargs) -> str:
        """ç”Ÿæˆ LLM å“åº”ï¼Œæ”¯æŒæµå¼è¾“å‡º"""
        
        if not self.llm_connector:
            self.logger.warning("LLM connector not available, using fallback")
            return f"LLMæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•ã€‚"
        
        try:
            if stream:
                return await self._generate_streaming_response(system_prompt, user_prompt)
            else:
                # ä½¿ç”¨ç°æœ‰çš„ LLM è¿æ¥å™¨ç”Ÿæˆå“åº”
                response, metadata = self.llm_connector.generate_response(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    use_cache=True
                )
                
                self.logger.debug(f"Generated response successfully")
                return response
            
        except Exception as e:
            self.logger.error(f"LLM generation failed: {e}")
            return f"ç”Ÿæˆå›å¤æ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯ä¿¡æ¯ï¼š{str(e)}"
    
    async def _generate_streaming_response(self, system_prompt: str, user_prompt: str) -> str:
        """æµå¼ç”Ÿæˆå“åº”"""
        try:
            print(f"\n{Colors.CYAN}ğŸ”® æ­£åœ¨ç”Ÿæˆè§£è¯»...{Colors.ENDC}\n")
            
            full_response = ""
            
            # ä½¿ç”¨è¿æ¥å™¨çš„æµå¼æ–¹æ³•
            for chunk in self.llm_connector.generate_response_streaming(system_prompt, user_prompt):
                full_response += chunk
                print(chunk, end='', flush=True)
            
            print()  # æ¢è¡Œ
            return full_response
            
        except Exception as e:
            self.logger.error(f"Streaming generation failed: {e}")
            # é™çº§åˆ°å¸¸è§„ç”Ÿæˆ
            response, metadata = self.llm_connector.generate_response(
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                use_cache=True
            )
            return response
